{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2871e609-ca2e-4f61-a0ef-46c2ff0d96ca",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dfe0868-7785-4d05-b39a-813bd8beaa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from src.model import unet_model\n",
    "from src.dataset import SegmentationDataset\n",
    "from src.data_pipeline import SegmentationDataPipeline\n",
    "from src.model_utils import (\n",
    "    CustomTensorBoard,\n",
    "    dice_coeff,\n",
    "    dice_loss,\n",
    "    bce_dice_loss,\n",
    "    tversky,\n",
    "    tversky_loss,\n",
    "    focal_tversky_loss,\n",
    "    evaluate_per_class_dice,\n",
    ")\n",
    "\n",
    "\n",
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a21e2d6-a452-484c-ad76-73c0ab283e3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data and create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb068c4a-7626-4578-b12c-60e9fc8f4aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUGMENTING!!\n",
      "Batching\n",
      "Batching\n"
     ]
    }
   ],
   "source": [
    "IMG_SHAPE = (256, 1600)\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "ANNOTATIONS_PATH = \"../data/train.csv\"\n",
    "TRAIN_IMG_PATH = \"../data/train_images/\"\n",
    "LOSSES = {\n",
    "    \"dice_loss\": dice_loss,\n",
    "    \"bce_dice_loss\": bce_dice_loss,\n",
    "    \"tversky_loss\": tversky_loss,\n",
    "    \"focal_tversky_loss\": focal_tversky_loss,\n",
    "}\n",
    "METRICS = {\n",
    "    \"dice_coeff\": dice_coeff,\n",
    "    \"tversky\": tversky,\n",
    "}\n",
    "LOG_DIR = f'logs/test{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "# instantiate dataset and pipelne\n",
    "# sd = SegmentationDataset(\n",
    "#     label_file=ANNOTATIONS_PATH,\n",
    "#     img_dir_path=TRAIN_IMG_PATH,\n",
    "#     img_shape=IMG_SHAPE,\n",
    "# )\n",
    "\n",
    "# create train/test & x/y splits\n",
    "train_imgs, test_imgs = sd.get_train_test_split(test_size=0.2)\n",
    "\n",
    "# small sample\n",
    "train_imgs = train_imgs[:16]\n",
    "test_imgs = test_imgs[:8]\n",
    "\n",
    "X_train = sd.get_image_sequence(train_imgs)\n",
    "y_train = sd.get_label_sequence(train_imgs, label_type=\"preprocessed\")\n",
    "X_test = sd.get_image_sequence(test_imgs)\n",
    "y_test = sd.get_label_sequence(test_imgs, label_type=\"preprocessed\")\n",
    "\n",
    "# create dataset pipelines\n",
    "sdp = SegmentationDataPipeline(\n",
    "    img_shape=IMG_SHAPE,\n",
    "    label_type=\"preprocessed\",\n",
    "    pipeline_options={\n",
    "        \"map_parallel\": None,\n",
    "        \"cache\": False,\n",
    "        \"shuffle_buffer_size\": False,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"prefetch\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "train_dataset = sdp(X_train, y_train, is_train=True)\n",
    "test_dataset = sdp(X_test, y_test, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cc1ac-5f06-4ccc-b848-4f071aa72650",
   "metadata": {},
   "source": [
    "## Define Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9020b813-7b6e-4508-a2af-91de01e1ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "unet = unet_model(IMG_SHAPE, n_channels_bottleneck=512 / 4)\n",
    "\n",
    "unet.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss=LOSSES[\"tversky_loss\"],\n",
    "    metrics=[dice_coeff, tversky],\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(LOG_DIR, \"best_model.h5\"), save_best_only=True\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4825991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 1.0, 1: 4.0, 2: 4.0, 3: 1.0, 4: 4.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "635b9790",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20108/1194328710.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m hist = unet.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_class_weights_map_fn\u001b[0;34m(*data)\u001b[0m\n\u001b[1;32m   1620\u001b[0m             )\n\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1622\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1623\u001b[0m             raise ValueError(\n\u001b[1;32m   1624\u001b[0m                 \u001b[0;34m\"`class_weight` not supported for \"\u001b[0m \u001b[0;34m\"3+ dimensional targets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'int'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "hist = unet.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_dataset,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837300b6",
   "metadata": {},
   "source": [
    "#### Without class weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "408afad4-93e6-49ad-9519-8b2362cde7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 851ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"logs/test20221017-185537/best_model.h5\"\n",
    "unet_model = tf.keras.models.load_model(MODEL_PATH, custom_objects=(LOSSES | METRICS))\n",
    "\n",
    "class_scores = evaluate_per_class_dice(test_dataset, unet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f1c824b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 3.6770607e-06,\n",
       " 1: 4.7923804e-10,\n",
       " 2: 1.8157777e-10,\n",
       " 3: 0.018686756,\n",
       " 4: 4.8614193e-11}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aacf274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
